현재 컨테이너 내에서 curl http://localhost:5000/health 정상적으로 수행됨
하지만 컨테이너를 실행하는 private subnet에서는 불가능
보안그룹 같은거 더 봐야할 듯??

-> 아 해결했다 sudo docker run -p 5000:5000 dminus251/test:latest 명령으로 포트포워딩하니까 됨
이제 private subnet 말고 인터넷에서도 접근할 수 있도록 해보자

그러면 이제 다시 eks 활성화해서 로드밸런서로 접근해야 함
그러면 ingress와 service가 필요하고, 내 이미지로 연결되도록 해야 함
일단 컨테이너 실행은 수동으로 해보자

헬스 체크: curl http://localhost:5000/health

CREATE: curl -X POST crud.dududrb.shop/items -H "Content-Type: application/json" -d '{"name": "test_item1"}'
CREATE: curl -X POST crud.dududrb.shop/items -H "Content-Type: application/json" -d '{"name": "test_item2"}'
READ:  curl crud.dududrb.shop/items
UPDATE: curl -X PUT crud.dududrb.shop/items/2 -H "Content-Type: application/json" -d '{"name": "updated_item2"} #id가 2인 item을 update
DELETE: curl -X DELETE crud.dududrb.shop/items/2

현재 ubuntu에서 pod 띄우고 DNS로 접근되는지 테스트 중
일단 pod namespace도 namespace로 옮겨야 함
그리고 t apply마다 route53 호스팅 대상 로드밸런서 다시 선택해줘야 함

private에선 docker run으로 테스트 완료 나중에 kubernete 이용해서 pod형태로 실행하자

route53 호스팅 대상 설정만 하면 끝날듯
ingress로 로드밸런서 생성 안 되면 aws-loadbalancer-controller pod 삭제하기

DB 콘솔에서 '구성'에 보조영역 화인 가능

****prometheus, grafana의 pvc가 pending인 경우
helm.tf를 나중에 apply하면 되는데 왜그런지 모르겠네
아마 addon부터 설치돼야하고, 그래서 helm의 depends_on에 addon을 추가해놨는데..
provisioner 문제는 아님
helm.tf를 나중에 추가하면 route table에서 nat_gateway_id 속성을 gateway_id 속성을 변경한다고 나옴 아마 이 문제가 아닐까??
내일 마저 해결해보자
-> 근본적 원인은 아닌듯?? helm.tf를 나중에 apply해도 여전히 pvc가 pending일 때도 있다

nat용 igw용 분리 후
1트: nat_gatewy_id만 채워져있고, helm.tf 변경 후 terraform apply해도 라우팅테이블 변경사항 없음, 성공
     pvc의 이 로그는 Bound 성공 시에도 나옴  Waiting for a volume to be created either by the external provisioner 'ebs.csi.aws.com' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
     gp2의 프로비저너는 kubernetes.io/aws-ebs인데 왜지?

내일 할 일
1. launch template 문제인지도 확인해보자 $Latest에서 1로 고정
2. 성공, 실패 시 ebs csi driver event 로그 확인해보기
3. 성공, 실패 시 콘솔의 EC2->EBS 차이점 있나 확인해보기

k describe로는 event에 이상 없고, k logs로 ebs csi driver 확인해보니까 아래 두 에러 메시지가 있음
  - could not create volume in EC2: operation error EC2: CreateVolume, get identity: get credentials: failed to refresh cached credentials, failed to retrieve credentials, operation error STS: AssumeRoleWithWebIdentity, exceeded maximum number of attempts, 3, https response error StatusCode: 0, RequestID: , request send failed, Post \"https://sts.ap-northeast-2.amazonaws.com/\": dial tcp: lookup sts.ap-northeast-2.amazonaws.com: i/o timeout"
  - could not create volume in EC2: operation error EC2: CreateVolume, get identity: get credentials: request canceled, context canceled"
그런데 이해가 안 가는 점은 private a subnet의 pod에서만 이런 로그가 있음 private c subnet의 로그는 에러 메시지 x
이중화 전에 private a subnet만 사용했을 땐 pending되는 에러가 발생하지 않았었는데 뭐지??
1. 권한 문제거나, 2. 네트워크 문제일 것임
에러가 발생했을 때 아래 순서로 프로비저닝됐음
  - eks cluster 생성
  - sg_rule-ng, sg_rule-cluster들 생성
  - update kubeconfig
  - oidc 생성
  - ebs-csi-controller용 Role 생성
  - ebs-csi-controller addon 생성
  - grafana, prometheus생성, 얘네가 pvc도 생성함 (persistentVolume = true이므로)
즉 oidc -> role -> addon -> pvc 순서로 생성하므로 Role에는 적절한 권한이 있음
그리고 private 2a 서브넷의 pod에서 에러가 발생하는데, 이 서브넷에 ssh 연결 후 ping 8.8.8.8 실행 결과 인터넷과 통신 가능함
즉 네트워크 문제도 아님 ...
또 콘솔에서 확인 결과 ebs는 생성되어 있고, 인스턴스에 붙어 있음
아니 근데 왜 저런 에러가 발생하지 ??????????

10/04
확인 결과 private-2a 서브넷의 노드 그룹에 있는 pod는 인터넷 접속이 안 되는 것 같음
kubectl log -o wide 명령으로 조회 시 private-2a 서브넷에 있는 ebs-csi-controller pod가 볼륨을 생성할 때만 pending 에러 발생
ubuntu pod를 실행해서 bash에 접속 후 apt install 명령이 실행되지 않음

ebs-csi-driver pod는 private-2c 서브넷의 노드 그룹에도 존재하지만, private-2a의 pod만 ebs 프로비저닝에 관여하는 문제는 둘째치고
일단 node affinity를 통해 ebs-csi-driver pod를 private-2c 서브넷에 있는 노드에 붙여서 5번 정도 테스트해보자
그리고 또 테스트해봤는데, private-2a 서브넷에 임의로 ec2를 생성해서 ssh로 접속해봤는데, 이 경우에는 인터넷에 접근이 가능함 (apt update 정상적으로 수행됨)
즉 클러스터와 관련해서 문제가 있어보인다.. storageclass의 provisioner가 문제가 아니였음

2024/10/04 13:54:12 http: TLS handshake error from 10.0.0.74:42788: read tcp 10.0.2.12:9443->10.0.0.74:42788: read: connection reset by peer 
42788 포트가 뭔지 확인 필요.. 쿠버네티스 문서에서는 이 포트에 관한 내용이 없음
https://kubernetes.io/docs/reference/networking/ports-and-protocols/

아 근데 이번엔 또 private-2a 서브넷의 ebs-csi-controller가 볼륨 프로비저닝해도 pending 안돼고 정상적으로 bound되네 
그렇다면 node affinity는 필요하지 않음

----------- pvc pending 문제는 해결됨 아마 storage class 문제였던 것 같다---------
# 두 번째: grafana가 prometheus 메트릭 수집 불가

yyk@localhost:~/rocking-terraform$ kubectl get svc -n kube-system
NAME                                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE

kube-dns                            ClusterIP   172.20.0.10      <none>   

grafana pod가 google에 curl을 못하는 문제 발생. 당연히 내가 배포한 clusterIP도 참조하지 못함
yyk@localhost:~/rocking-terraform$ kubectl exec -it practice-grafana-974b644dd-nzwwn -n monitoring -- curl -I https://www.google.com
curl: (6) Could not resolve host: www.google.com
command terminated with exit code 6

##pod는 /etc/resolve.conf의 네임서버를 참조해 도메인을 질의한다. 아래 로그를 보면, 네임서버는 정상적으로 kube-dns 서비스의 cluster-ip를 참조하는 것을 확인함

yyk@localhost:~/rocking-terraform$ kubectl exec -it practice-grafana-974b644dd-nzwwn -n monitoring -- cat /etc/resolv.conf
search monitoring.svc.cluster.local svc.cluster.local cluster.local ap-northeast-2.compute.internal
nameserver 172.20.0.10
options ndots:5

yyk@localhost:~/rocking-terraform$ k get po -o wide -n monitoring
NAME                                                          READY   STATUS    RESTARTS   AGE   IP           NODE                                           NOMINATED NODE   READINESS GATES
pod-crud                                                      1/1     Running   0          25m   10.0.0.179   ip-10-0-0-76.ap-northeast-2.compute.internal   <none>           <none>
practice-grafana-974b644dd-nzwwn                              1/1     Running   0          26m   10.0.2.133   ip-10-0-2-93.ap-northeast-2.compute.internal   <none>           <none>
