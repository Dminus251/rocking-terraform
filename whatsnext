현재 컨테이너 내에서 curl http://localhost:5000/health 정상적으로 수행됨
하지만 컨테이너를 실행하는 private subnet에서는 불가능
보안그룹 같은거 더 봐야할 듯??

-> 아 해결했다 sudo docker run -p 5000:5000 dminus251/test:latest 명령으로 포트포워딩하니까 됨
이제 private subnet 말고 인터넷에서도 접근할 수 있도록 해보자

그러면 이제 다시 eks 활성화해서 로드밸런서로 접근해야 함
그러면 ingress와 service가 필요하고, 내 이미지로 연결되도록 해야 함
일단 컨테이너 실행은 수동으로 해보자

헬스 체크: curl http://localhost:5000/health

CREATE: curl -X POST crud.dududrb.shop/items -H "Content-Type: application/json" -d '{"name": "test_item1"}'
CREATE: curl -X POST crud.dududrb.shop/items -H "Content-Type: application/json" -d '{"name": "test_item2"}'
READ:  curl crud.dududrb.shop/items
UPDATE: curl -X PUT crud.dududrb.shop/items/2 -H "Content-Type: application/json" -d '{"name": "updated_item2"} #id가 2인 item을 update
DELETE: curl -X DELETE crud.dududrb.shop/items/2

현재 ubuntu에서 pod 띄우고 DNS로 접근되는지 테스트 중
일단 pod namespace도 namespace로 옮겨야 함
그리고 t apply마다 route53 호스팅 대상 로드밸런서 다시 선택해줘야 함

private에선 docker run으로 테스트 완료 나중에 kubernete 이용해서 pod형태로 실행하자

route53 호스팅 대상 설정만 하면 끝날듯
ingress로 로드밸런서 생성 안 되면 aws-loadbalancer-controller pod 삭제하기

DB 콘솔에서 '구성'에 보조영역 화인 가능

****prometheus, grafana의 pvc가 pending인 경우
helm.tf를 나중에 apply하면 되는데 왜그런지 모르겠네
아마 addon부터 설치돼야하고, 그래서 helm의 depends_on에 addon을 추가해놨는데..
provisioner 문제는 아님
helm.tf를 나중에 추가하면 route table에서 nat_gateway_id 속성을 gateway_id 속성을 변경한다고 나옴 아마 이 문제가 아닐까??
내일 마저 해결해보자
-> 근본적 원인은 아닌듯?? helm.tf를 나중에 apply해도 여전히 pvc가 pending일 때도 있다

nat용 igw용 분리 후
1트: nat_gatewy_id만 채워져있고, helm.tf 변경 후 terraform apply해도 라우팅테이블 변경사항 없음, 성공
     pvc의 이 로그는 Bound 성공 시에도 나옴  Waiting for a volume to be created either by the external provisioner 'ebs.csi.aws.com' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
     gp2의 프로비저너는 kubernetes.io/aws-ebs인데 왜지?

내일 할 일
1. launch template 문제인지도 확인해보자 $Latest에서 1로 고정
2. 성공, 실패 시 ebs csi driver event 로그 확인해보기
3. 성공, 실패 시 콘솔의 EC2->EBS 차이점 있나 확인해보기

k describe로는 event에 이상 없고, k logs로 ebs csi driver 확인해보니까 아래 두 에러 메시지가 있음
  - could not create volume in EC2: operation error EC2: CreateVolume, get identity: get credentials: failed to refresh cached credentials, failed to retrieve credentials, operation error STS: AssumeRoleWithWebIdentity, exceeded maximum number of attempts, 3, https response error StatusCode: 0, RequestID: , request send failed, Post \"https://sts.ap-northeast-2.amazonaws.com/\": dial tcp: lookup sts.ap-northeast-2.amazonaws.com: i/o timeout"
  - could not create volume in EC2: operation error EC2: CreateVolume, get identity: get credentials: request canceled, context canceled"
그런데 이해가 안 가는 점은 private a subnet의 pod에서만 이런 로그가 있음 private c subnet의 로그는 에러 메시지 x
이중화 전에 private a subnet만 사용했을 땐 pending되는 에러가 발생하지 않았었는데 뭐지??
1. 권한 문제거나, 2. 네트워크 문제일 것임
에러가 발생했을 때 아래 순서로 프로비저닝됐음
  - eks cluster 생성
  - sg_rule-ng, sg_rule-cluster들 생성
  - update kubeconfig
  - oidc 생성
  - ebs-csi-controller용 Role 생성
  - ebs-csi-controller addon 생성
  - grafana, prometheus생성, 얘네가 pvc도 생성함 (persistentVolume = true이므로)
즉 oidc -> role -> addon -> pvc 순서로 생성하므로 Role에는 적절한 권한이 있음
그리고 private 2a 서브넷의 pod에서 에러가 발생하는데, 이 서브넷에 ssh 연결 후 ping 8.8.8.8 실행 결과 인터넷과 통신 가능함
즉 네트워크 문제도 아님 ...
또 콘솔에서 확인 결과 ebs는 생성되어 있고, 인스턴스에 붙어 있음
아니 근데 왜 저런 에러가 발생하지 ??????????

10/04
확인 결과 private-2a 서브넷의 노드 그룹에 있는 pod는 인터넷 접속이 안 되는 것 같음
kubectl log -o wide 명령으로 조회 시 private-2a 서브넷에 있는 ebs-csi-controller pod가 볼륨을 생성할 때만 pending 에러 발생
ubuntu pod를 실행해서 bash에 접속 후 apt install 명령이 실행되지 않음

ebs-csi-driver pod는 private-2c 서브넷의 노드 그룹에도 존재하지만, private-2a의 pod만 ebs 프로비저닝에 관여하는 문제는 둘째치고
일단 node affinity를 통해 ebs-csi-driver pod를 private-2c 서브넷에 있는 노드에 붙여서 5번 정도 테스트해보자
그리고 또 테스트해봤는데, private-2a 서브넷에 임의로 ec2를 생성해서 ssh로 접속해봤는데, 이 경우에는 인터넷에 접근이 가능함 (apt update 정상적으로 수행됨)
즉 클러스터와 관련해서 문제가 있어보인다.. storageclass의 provisioner가 문제가 아니였음

2024/10/04 13:54:12 http: TLS handshake error from 10.0.0.74:42788: read tcp 10.0.2.12:9443->10.0.0.74:42788: read: connection reset by peer 
42788 포트가 뭔지 확인 필요.. 쿠버네티스 문서에서는 이 포트에 관한 내용이 없음
https://kubernetes.io/docs/reference/networking/ports-and-protocols/

아 근데 이번엔 또 private-2a 서브넷의 ebs-csi-controller가 볼륨 프로비저닝해도 pending 안돼고 정상적으로 bound되네 
그렇다면 node affinity는 필요하지 않음

----------- pvc pending 문제는 해결됨 아마 storage class 문제였던 것 같다---------
# 두 번째: grafana가 prometheus 메트릭 수집 불가

yyk@localhost:~/rocking-terraform$ kubectl get svc -n kube-system
NAME                                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE

kube-dns                            ClusterIP   172.20.0.10      <none>   

grafana pod가 google에 curl을 못하는 문제 발생. 당연히 내가 배포한 clusterIP도 참조하지 못함
yyk@localhost:~/rocking-terraform$ kubectl exec -it practice-grafana-974b644dd-nzwwn -n monitoring -- curl -I https://www.google.com
curl: (6) Could not resolve host: www.google.com
command terminated with exit code 6

##pod는 /etc/resolve.conf의 네임서버를 참조해 도메인을 질의한다. 아래 로그를 보면, 네임서버는 정상적으로 kube-dns 서비스의 cluster-ip를 참조하는 것을 확인함

yyk@localhost:~/rocking-terraform$ kubectl exec -it practice-grafana-974b644dd-nzwwn -n monitoring -- cat /etc/resolv.conf
search monitoring.svc.cluster.local svc.cluster.local cluster.local ap-northeast-2.compute.internal
nameserver 172.20.0.10
options ndots:5

yyk@localhost:~/rocking-terraform$ k get po -o wide -n monitoring
NAME                                                          READY   STATUS    RESTARTS   AGE   IP           NODE                                           NOMINATED NODE   READINESS GATES
pod-crud                                                      1/1     Running   0          25m   10.0.0.179   ip-10-0-0-76.ap-northeast-2.compute.internal   <none>           <none>
practice-grafana-974b644dd-nzwwn                              1/1     Running   0          26m   10.0.2.133   ip-10-0-2-93.ap-northeast-2.compute.internal   <none>           <none>

노드의 보안 그룹에서도 53 포트를 허용해야 하나?
  -> 그럴 필요는 없어보임 이미 core-dns라는 서비스가 배포되어 있어서 coredns pod에 접근이 가능함
그렇다면 curl google.com이 실패하는 건 인터넷 연결 문제일 가능성이 큼.. 네트워크 부분을 더 확인해보자

gpt 답변이긴 하지만
외부 dns 요청의 경우 노드 그룹에서 53번 포트를 허용해야 한다고 함
pod의 sh에선 apt update 명령이 실행되지 않고, pod와 같은 서브넷에 직접 ec2를 생성하면 apt update 명령이 실행되는 걸 보아
아마 이 경우에 속하지 않을까 싶음
내일 다시 
  1) 일단 현재 코드로 pod에서 apt update 해보고
  2) 노드 그룹에 53 허용해서 apt update 해보자

근데 하긴 클러스터 보안 그룹은 control plane과 cluster resource 간의 통신을 제어하니까
노드 그룹의 보안 그룹에 53을 허용하는 게 더 그럴듯하네
긜고 pvc Bound된 경우 ebs-csi-controller 두 개 logs로 뭐가 리더 pod인지 확인해보자
일단 10.0.0.0/24인 경우엔 계속 pending되는 것 같음
node끼리 모든 포트를 노출하는 방법은 없나 이것도 확인해보자

NAME                                            READY   STATUS    RESTARTS   AGE   IP           NODE                                            NOMINATED NODE   READINESS GATES
ebs-csi-controller-76747c7b84-dbdl8             6/6     Running   0          17m   10.0.0.108   ip-10-0-0-213.ap-northeast-2.compute.internal   <none>           <none>
ebs-csi-controller-76747c7b84-lvndl             6/6     Running   0          17m   10.0.2.73    ip-10-0-2-124.ap-northeast-2.compute.internal   <none>           <none>
ebs-csi-node-9g7q2                              3/3     Running   0          17m   10.0.0.166   ip-10-0-0-213.ap-northeast-2.compute.internal   <none>           <none>
ebs-csi-node-zjrn7                              3/3     Running   0          17m   10.0.2.232   ip-10-0-2-124.ap-northeast-2.compute.internal   <none>           <none>

10.0.0.0/24 로그
yyk@localhost:~/practice_terraform/demo10-docker$ k logs ebs-csi-controller-76747c7b84-dbdl8 -n kube-system
Defaulted container "ebs-plugin" out of: ebs-plugin, csi-provisioner, csi-attacher, csi-snapshotter, csi-resizer, liveness-probe
I1007 01:08:57.039760       1 main.go:151] "Region provided via AWS_REGION environment variable" region="ap-northeast-2"
I1007 01:08:57.041513       1 driver.go:69] "Driver Information" Driver="ebs.csi.aws.com" Version="v1.35.0"
E1007 01:10:50.036676       1 driver.go:108] "GRPC error" err="rpc error: code = Internal desc = Could not create volume \"pvc-1826038d-6cc9-4a11-a1cc-725540ca9661\": could not create volume in EC2: operation error EC2: CreateVolume, https response error StatusCode: 0, RequestID: , canceled, context canceled"
E1007 01:10:54.499859       1 driver.go:108] "GRPC error" err="rpc error: code = Internal desc = Could not create volume \"pvc-54aad6f0-03c9-481a-91ea-0fb29b9983e4\": could not create volume in EC2: operation error EC2: CreateVolume, https response error StatusCode: 0, RequestID: , canceled, context canceled"
E1007 01:10:54.902400       1 driver.go:108] "GRPC error" err="rpc error: code = Internal desc = Could not create volume \"pvc-85a722b4-e6c1-4472-b906-6a1dcb14e983\": could not create volume in EC2: operation error EC2: CreateVolume, https response error StatusCode: 0, RequestID: , canceled, context canceled"
I1007 01:11:45.140391       1 controller.go:415] "ControllerPublishVolume: attaching" volumeID="vol-0788d8434caa81b93" nodeID="i-090ecfbb8fe61c637"
I1007 01:11:46.457494       1 cloud.go:1106] "Waiting for volume state" volumeID="vol-0788d8434caa81b93" actual="attaching" desired="attached"
I1007 01:11:47.104297       1 controller.go:415] "ControllerPublishVolume: attaching" volumeID="vol-04f849e516eec250a" nodeID="i-0ad8f5323e3335265"
I1007 01:11:48.065566       1 controller.go:424] "ControllerPublishVolume: attached" volumeID="vol-0788d8434caa81b93" nodeID="i-090ecfbb8fe61c637" devicePath="/dev/xvdaa"
I1007 01:11:48.728670       1 controller.go:424] "ControllerPublishVolume: attached" volumeID="vol-04f849e516eec250a" nodeID="i-0ad8f5323e3335265" devicePath="/dev/xvdaa"
I1007 01:11:48.738753       1 controller.go:415] "ControllerPublishVolume: attaching" volumeID="vol-04f849e516eec250a" nodeID="i-0ad8f5323e3335265"
I1007 01:11:49.902486       1 controller.go:424] "ControllerPublishVolume: attached" volumeID="vol-04f849e516eec250a" nodeID="i-0ad8f5323e3335265" devicePath="/dev/xvdaa"
I1007 01:11:51.170595       1 controller.go:415] "ControllerPublishVolume: attaching" volumeID="vol-09304502c56df0885" nodeID="i-0ad8f5323e3335265"
I1007 01:11:52.781328       1 controller.go:424] "ControllerPublishVolume: attached" volumeID="vol-09304502c56df0885" nodeID="i-0ad8f5323e3335265" devicePath="/dev/xvdab"
I1007 01:11:52.791083       1 controller.go:415] "ControllerPublishVolume: attaching" volumeID="vol-09304502c56df0885" nodeID="i-0ad8f5323e3335265"
I1007 01:11:53.986918       1 controller.go:424] "ControllerPublishVolume: attached" volumeID="vol-09304502c56df0885" nodeID="i-0ad8f5323e3335265" devicePath="/dev/xvdab"

10.0.2.0/24 로그
yyk@localhost:~/practice_terraform/demo10-docker$ k logs ebs-csi-controller-76747c7b84-lvndl -n kube-system
Defaulted container "ebs-plugin" out of: ebs-plugin, csi-provisioner, csi-attacher, csi-snapshotter, csi-resizer, liveness-probe
I1007 01:08:57.621446       1 main.go:151] "Region provided via AWS_REGION environment variable" region="ap-northeast-2"
I1007 01:08:57.623182       1 driver.go:69] "Driver Information" Driver="ebs.csi.aws.com" Version="v1.35.0"

10/7
그냥 10.0.0.0/24의 pod는 인터넷과 통신되지 않는 것을 확인함
10.0.0.0/24에 직접 ec2를 생성하면 인터넷과 통신이 되는 것도 확인함
즉 pod가 인터넷과 통신 불가능 ..
또 clusterIP 못 찾던 것도 생각해야 함
sudo scp -i 0827.pem ./0827.pem ubuntu@52.78.118.66:/home/ubuntu/

+ 추가
이번엔 10.0.0.0/24의 pod는 되는데 10.0.2.0/24가 안 되네
terraform apply마다 이렇게 설정이 바뀌는 이유가 뭐지?
관련 로그

10.0.0.0/24
yyk@localhost:~$ k exec -it ubuntu-2a -- /bin/bash
root@ubuntu-2a:/# apt update
Get:1 http://archive.ubuntu.com/ubuntu noble InRelease [256 kB]
Get:2 http://security.ubuntu.com/ubuntu noble-security InRelease [126 kB]
(중간 생략)
Get:16 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 Packages [682 kB]
Get:17 http://archive.ubuntu.com/ubuntu noble-backports/universe amd64 Packages [11.8 kB]
Fetched 25.2 MB in 5s (4891 kB/s)
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
2 packages can be upgraded. Run 'apt list --upgradable' to see them.

10.0.2.0/24
yyk@localhost:~/rocking-terraform$ k exec -it ubuntu-2c -- /bin/bash
root@ubuntu-2c:/# apt update
Ign:1 http://security.ubuntu.com/ubuntu noble-security InRelease
(중간 생략)
Err:1 http://security.ubuntu.com/ubuntu noble-security InRelease
  Temporary failure resolving 'security.ubuntu.com'
Err:2 http://archive.ubuntu.com/ubuntu noble InRelease
  Temporary failure resolving 'archive.ubuntu.com'
Err:3 http://archive.ubuntu.com/ubuntu noble-updates InRelease
  Temporary failure resolving 'archive.ubuntu.com'
Err:4 http://archive.ubuntu.com/ubuntu noble-backports InRelease
  Temporary failure resolving 'archive.ubuntu.com'
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
All packages are up to date.
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/noble/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/noble-updates/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/noble-backports/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://security.ubuntu.com/ubuntu/dists/noble-security/InRelease  Temporary failure resolving 'security.ubuntu.com'
W: Some index files failed to download. They have been ignored, or old ones used instead.

10/08
DNS CONFIG에 nameserver로 8.8.8.8 추가했더니 private-2a와 private-2c 모두에서 apt update 성공함
일단 3번 정도 더 terraform apply 하면서 테스트해보자
그리고 아마 prometheus, grafana, crud, ebs-csi-controller에서도 필요함
얘네도 각각 들어가봐서 /etc/resolve.conf가 있는지 확인해보기
ebs는 addon인데 helm으로 바꿔야 하나? (addon은 dns_config 수정 불가)

dns_config 설정하지 않은 우분투의 /etc/resolve.conf 내용은 아래와 같음
search default.svc.cluster.local svc.cluster.local cluster.local ap-northeast-2.compute.internal
nameserver 172.20.0.10
options ndots:5

클러스터와 통신하려면 prometheus, crud에 이 내용도 추가해야 할 것 같다
prometheus-server는 /bin/sh 사용해야 함

드디어 성공!! 결국 dns 문제였네
grafana pod에 접속해서 아래 명령 중 하나 입력해서 테스트
  - curl http://practice-clusterip.monitoring.svc.cluster.local:9090
  - curl http://{service_ip}:9090
/metrics 경로에 curl 보내면 메트릭 얻어올 수 있음
